{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FRAMES = 4\n",
    "\n",
    "def filter(obs):\n",
    "    m = np.mean(obs)\n",
    "    v = np.var(obs)\n",
    "    \n",
    "    obs = (obs - m)/v   \n",
    "    return obs\n",
    "\n",
    "def get_stacked_obs(obs, prev_frames):\n",
    "    if not prev_frames:\n",
    "        prev_frames = [obs] * (N_FRAMES - 1)\n",
    "        \n",
    "    prev_frames.append(obs)\n",
    "\n",
    "    stacked_frames = np.stack(prev_frames)\n",
    "    prev_frames = prev_frames[-(N_FRAMES-1):]\n",
    "    \n",
    "    return stacked_frames, prev_frames\n",
    "\n",
    "def preprocess_obs(obs, prev_frames):\n",
    "    filtered_obs = filter(obs)\n",
    "    stacked_obs, prev_frames = get_stacked_obs(filtered_obs, prev_frames)\n",
    "    return stacked_obs, prev_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.data = []\n",
    "        \n",
    "    def add_step(self, step_data):\n",
    "        self.data.append(step_data)\n",
    "        if len(self.data) > self.capacity:\n",
    "            self.data = self.data[-self.capacity:]\n",
    "            \n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.data))\n",
    "        indices = np.random.choice(range(len(self.data)), n, replace=False)\n",
    "        samples = np.asarray(self.data, dtype=\"object\")[indices]\n",
    "\n",
    "        #input = torch.tensor(np.concatenate((samples[:, 0][0],list(samples[:, 1])*4),axis=1)).float() \n",
    "      \n",
    "        state_data = torch.tensor(np.stack(samples[:, 0])).float()\n",
    "        act_data = torch.tensor(np.stack(samples[:, 1])).float()\n",
    "        reward_data = torch.tensor(np.stack(samples[:, 2])).float()\n",
    "        next_state_data = torch.tensor(np.stack(samples[:, 3])).float()\n",
    "        terminal_data = torch.tensor(np.stack(samples[:, 4])).int()\n",
    "        \n",
    "        return state_data, act_data, reward_data, next_state_data, terminal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, len_state, len_action):\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(len_state+len_action,64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(64,1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        q_values = self.layer1(obs)\n",
    "        q_values = self.layer2(q_values)\n",
    "        q_values = self.layer3(q_values)\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def train_on_batch(self, target_model, optimizer,input, obs, acts, rewards, next_obs, terminals, gamma=0.99):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<model.NN.NN_model object at 0x28d42b0e0>\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(-3,3,90).reshape(-1,3)\n",
    "y = np.sum(np.sin(X),axis=1)\n",
    "\n",
    "def f(x):\n",
    "    s = np.sin(x).shape\n",
    "    if len(s)>1:\n",
    "        return np.sum(np.sin(x),axis=1)\n",
    "    else:\n",
    "        return [np.sum(np.sin(x))]\n",
    "\n",
    "l_bounds = [-3,-3,-3]\n",
    "u_bounds = [3,3,3]\n",
    "\n",
    "env = Environment(X = X,y = y,l_bounds = l_bounds,u_bounds = u_bounds, func = f, model = 'NN',model_param = {'d':3,'nb_nodes':40,'nb_layers':4})\n",
    "\n",
    "state = env.Reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "max_steps = 100\n",
    "er_capacity = 15 # 1m in paper\n",
    "\n",
    "train_batch_size = 32\n",
    "learning_rate = 2.5e-4\n",
    "update_freq = 4\n",
    "print_freq = 5\n",
    "frame_skip = 3\n",
    "n_anneal_steps = 1e2 \n",
    "target_update_delay = 10 \n",
    "epsilon = lambda step: np.clip(1 - 0.9 * (step/n_anneal_steps), 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "Episode #0 | Step #1 | Epsilon 0.99 | Avg. Reward 0.00\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "Episode #5 | Step #6 | Epsilon 0.95 | Avg. Reward 0.00\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "Episode #10 | Step #11 | Epsilon 0.90 | Avg. Reward 0.00\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "Episode #15 | Step #16 | Epsilon 0.86 | Avg. Reward 0.00\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "Episode #20 | Step #21 | Epsilon 0.81 | Avg. Reward 0.00\n",
      "random\n",
      "not random\n",
      "random\n",
      "random\n",
      "random\n",
      "Episode #25 | Step #26 | Epsilon 0.77 | Avg. Reward 0.00\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "Episode #30 | Step #31 | Epsilon 0.72 | Avg. Reward 0.00\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "random\n",
      "Episode #35 | Step #37 | Epsilon 0.67 | Avg. Reward 0.00\n",
      "random\n",
      "random\n",
      "random\n",
      "not random\n",
      "random\n",
      "Episode #40 | Step #42 | Epsilon 0.62 | Avg. Reward 0.00\n",
      "not random\n",
      "random\n",
      "random\n",
      "not random\n",
      "random\n",
      "not random\n",
      "Episode #45 | Step #48 | Epsilon 0.57 | Avg. Reward 0.00\n",
      "random\n",
      "not random\n",
      "not random\n",
      "random\n",
      "random\n",
      "Episode #50 | Step #53 | Epsilon 0.52 | Avg. Reward 0.00\n",
      "not random\n",
      "not random\n",
      "not random\n",
      "random\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m cumulative_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frame_skip):\n\u001b[0;32m---> 42\u001b[0m     next_obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mStep(act)\n\u001b[1;32m     43\u001b[0m     cumulative_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     44\u001b[0m     current_reward \u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Desktop/rl/environment.py:96\u001b[0m, in \u001b[0;36mEnvironment.Step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX,[a]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my,[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(a)]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m _,model_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#self.model.fit(self.X,self.y)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter:\n",
      "File \u001b[0;32m~/Desktop/rl/model/NN.py:61\u001b[0m, in \u001b[0;36mNN_model.fit\u001b[0;34m(self, X, y, n_epochs, batch_size, first)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X), batch_size):\n\u001b[1;32m     60\u001b[0m     Xbatch \u001b[38;5;241m=\u001b[39m X[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 61\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximation_nn(Xbatch)\n\u001b[1;32m     62\u001b[0m     ybatch \u001b[38;5;241m=\u001b[39m y[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y_pred, ybatch)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "er = ExperienceReplay(er_capacity)\n",
    "model = DQN(len(state),X.shape[1])\n",
    "\n",
    "target_model = copy.deepcopy(model)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, eps=1e-6)\n",
    "all_rewards = []\n",
    "global_step = 0\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    prev_frames = []\n",
    "    obs, prev_frames = preprocess_obs(env.Reset(), prev_frames)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "\n",
    "        ### Epsilon - greedy ###\n",
    "        if np.random.rand() < epsilon(global_step):\n",
    "            sampler = qmc.LatinHypercube(d=X.shape[1])\n",
    "            sample = sampler.random(n=1)\n",
    "            sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "            act = np.array(sample_scaled)[0]\n",
    "            print('random')\n",
    "           \n",
    "        else:   \n",
    "            sampler = qmc.LatinHypercube(d=X.shape[1])\n",
    "            sample = sampler.random(n=5)\n",
    "            sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "            acts = np.array(sample_scaled)\n",
    "            q_values = []\n",
    "            for a in acts:\n",
    "                input = torch.tensor(np.concatenate((obs,[a]*4),axis=1)).float() \n",
    "                q_value = model(input)[0]\n",
    "                q_value = q_value.detach().numpy()\n",
    "                q_values.append(q_value)\n",
    "                \n",
    "            act = acts[np.argmax(q_values)]\n",
    "            print('not random')\n",
    "        \n",
    "        cumulative_reward = 0\n",
    "        for _ in range(frame_skip):\n",
    "            next_obs, reward, done, _ = env.Step(act)\n",
    "            cumulative_reward += reward\n",
    "            current_reward = reward\n",
    "            if done or step >= max_steps:\n",
    "                break\n",
    "        #episode_reward += cumulative_reward\n",
    "        #reward = format_reward(cumulative_reward)\n",
    "        reward = current_reward\n",
    "\n",
    "        next_obs, prev_frames = preprocess_obs(next_obs, prev_frames)\n",
    "        er.add_step([obs, act, reward, next_obs, int(done)])\n",
    "        obs = next_obs\n",
    "        \n",
    "        ### Train on a minibatch ###\n",
    "        \n",
    "        if global_step % update_freq == 0:\n",
    "            obs_data, act_data, reward_data, next_obs_data, terminal_data = er.sample(train_batch_size)\n",
    "            #print(obs_data, act_data)\n",
    "            model.train_on_batch(target_model, optimizer, input, obs_data, act_data,\n",
    "                                 reward_data, next_obs_data, terminal_data)\n",
    "        \n",
    "        ### Update target network ###\n",
    "        \n",
    "        if global_step and global_step % target_update_delay == 0:\n",
    "            target_model = copy.deepcopy(model)\n",
    "        \n",
    "        ### Finish the step ###\n",
    "        \n",
    "        step += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    all_rewards.append(episode_reward)\n",
    "    \n",
    "    if episode % print_freq == 0:\n",
    "        print('Episode #{} | Step #{} | Epsilon {:.2f} | Avg. Reward {:.2f}'.format(\n",
    "            episode, global_step, epsilon(global_step), np.mean(all_rewards[-print_freq:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
